{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use spacy to split sentences\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def sentence_splitter(text):\n",
    "    return [sent.strip() for sent in text.split('.') if sent != '' and len(sent.split(' ')) > 3 ]\n",
    "\n",
    "\n",
    "def construct_nli_dataset(sample, intervention):\n",
    "    subject_ground_truth = sample['dependancies']['subject_entity']['ground_truth']\n",
    "    subject_ground_truth_string = [f\"{sample['requested_rewrite']['subject']} {key} {', '.join(value)}\" for key,value in subject_ground_truth.items()][:4]\n",
    "    related_entity_ground_truth = sample['dependancies']['coupled_entities'][0]['ground_truth']\n",
    "    related_entity_ground_truth_string = [f\"{sample['requested_rewrite']['subject']} {key} {', '.join(value)}\" for key,value in related_entity_ground_truth.items()][:4]\n",
    "\n",
    "    new_fact = sample[\"requested_rewrite\"][\"prompt\"].format(\n",
    "            sample[\"requested_rewrite\"]['subject']\n",
    "        ) + \" \" + sample[\"requested_rewrite\"]['target_new']['str']\n",
    "    passage_of_text_about_subject_of_edit = sample['subject_prompt'].strip().replace('\\n', ' ')\n",
    "    passage_of_text_about_related_entity = sample['coupled_prompt'].strip().replace('\\n', ' ')\n",
    "    main_text_segmented = sentence_splitter(passage_of_text_about_subject_of_edit)\n",
    "    related_text_segmented = sentence_splitter(passage_of_text_about_related_entity)\n",
    "\n",
    "    sample_dataset_records = []\n",
    "    for sent in main_text_segmented:\n",
    "        sample_dataset_records.append({\n",
    "            \"content\": f\"Sentence 1: {new_fact} \\n\\n Sentence 2: {sent}\",\n",
    "            \"sample\": get_sample_id(sample),\n",
    "            \"intervention\": intervention,\n",
    "            \"label\": \"new_fact_and_main_passage\"\n",
    "        })\n",
    "    for sent in related_text_segmented:\n",
    "        sample_dataset_records.append({\n",
    "            \"content\": f\"Sentence 1: {new_fact} \\n\\n Sentence 2: {sent}\",\n",
    "            \"sample\": get_sample_id(sample),\n",
    "            \"intervention\": intervention,\n",
    "            \"label\": \"new_fact_and_related_passage\"\n",
    "        })\n",
    "    for sent in main_text_segmented:\n",
    "        for ground_truth in subject_ground_truth_string:\n",
    "            sample_dataset_records.append({\n",
    "                \"content\": f\"Sentence 1: {ground_truth} \\n\\n Sentence 2: {sent}\",\n",
    "                \"sample\": get_sample_id(sample),\n",
    "                \"intervention\": intervention,\n",
    "                \"label\": \"ground_truth_and_main_passage\"\n",
    "            })\n",
    "    for sent in related_text_segmented:\n",
    "        for ground_truth in related_entity_ground_truth_string:\n",
    "            sample_dataset_records.append({\n",
    "                \"content\": f\"Sentence 1: {ground_truth} \\n\\n Sentence 2: {sent}\",\n",
    "                \"sample\": get_sample_id(sample),\n",
    "                \"intervention\": intervention,\n",
    "                \"label\": \"ground_truth_and_related_passage\"\n",
    "            })\n",
    "        \n",
    "    sentence_pairs = []\n",
    "    for sent_1 in main_text_segmented:\n",
    "        for sent_2 in main_text_segmented:\n",
    "            if sent_1 != sent_2 and (sent_1, sent_2) not in sentence_pairs:\n",
    "                sample_dataset_records.append({\n",
    "                    \"content\": f\"Sentence 1: {sent_1} \\n\\n Sentence 2: {sent_2}\",\n",
    "                    \"sample\": get_sample_id(sample),\n",
    "                    \"intervention\": intervention,\n",
    "                    \"label\": \"main_passage_consistency\"\n",
    "                })\n",
    "                sentence_pairs.append((sent_1, sent_2))\n",
    "                sentence_pairs.append((sent_2, sent_1))\n",
    "    \n",
    "    sentence_pairs = []\n",
    "    for sent_1 in related_text_segmented:\n",
    "        for sent_2 in related_text_segmented:\n",
    "            if sent_1 != sent_2 and (sent_1, sent_2) not in sentence_pairs:\n",
    "                sample_dataset_records.append({\n",
    "                    \"content\": f\"Sentence 1: {sent_1} \\n\\n Sentence 2: {sent_2}\",\n",
    "                    \"sample\": get_sample_id(sample),\n",
    "                    \"intervention\": intervention,\n",
    "                    \"label\": \"related_passage_consistency\"\n",
    "                })\n",
    "                sentence_pairs.append((sent_1, sent_2))\n",
    "                sentence_pairs.append((sent_2, sent_1))\n",
    "\n",
    "    sentence_pairs = []\n",
    "    for sent_1 in main_text_segmented:\n",
    "        for sent_2 in related_text_segmented:\n",
    "            if (sent_1, sent_2) not in sentence_pairs:\n",
    "                sample_dataset_records.append({\n",
    "                    \"content\": f\"Sentence 1: {sent_1} \\n\\n Sentence 2: {sent_2}\",\n",
    "                    \"sample\": get_sample_id(sample),\n",
    "                    \"intervention\": intervention,\n",
    "                    \"label\": \"main_passage_and_related_passage_consistency\"\n",
    "                })\n",
    "                sentence_pairs.append((sent_1, sent_2))\n",
    "                sentence_pairs.append((sent_2, sent_1))\n",
    "\n",
    "    return sample_dataset_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "from src.utils import get_sample_id\n",
    "from src.construct_samples import (\n",
    "    NEW_FACT_TEMPLATE,\n",
    "    RELATED_ENTITY_TEMPLATE,\n",
    "    MAIN_PASSAGE_TEMPLATE_WITHOUT,\n",
    "    OLD_FACTS_SUBJECT_TEMPLATE,\n",
    "    RELATED_PASSAGE_TEMPLATE_WITHOUT,\n",
    "    OLD_FACTS_RELATED_TEMPLATE,\n",
    "    get_sample_text\n",
    ")\n",
    "\n",
    "from src.prompts import (\n",
    "    INSTRUCTION_PROMPT,\n",
    "    SURVEY_EXAMPLES,\n",
    "    SURVEY_ITEMS\n",
    ")\n",
    "\n",
    "survey_header = INSTRUCTION_PROMPT + \"\".join(SURVEY_EXAMPLES.values())\n",
    "survey_footer = \"\".join(SURVEY_ITEMS.values())\n",
    "\n",
    "\n",
    "\n",
    "def get_json_files(path):\n",
    "    samples = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".json\"):\n",
    "                # open file and append to samples\n",
    "                with open(os.path.join(root, file), 'r') as f:\n",
    "                    samples.append(json.load(f))\n",
    "    return samples\n",
    "\n",
    "rome_edit_dir = '../data/survey_samples/rome'\n",
    "no_edit_dir = '../data/survey_samples/no_edit'\n",
    "human_dir = '../data/survey_samples/human'\n",
    "\n",
    "rome_edit_files = get_json_files(rome_edit_dir)\n",
    "no_edit_files = get_json_files(no_edit_dir)\n",
    "human_files = get_json_files(human_dir)\n",
    "\n",
    "samples = []\n",
    "for file in rome_edit_files:\n",
    "    samples.extend(construct_nli_dataset(file, 'rome'))\n",
    "for file in no_edit_files:\n",
    "    samples.extend(construct_nli_dataset(file, 'no_edit'))\n",
    "for file in human_files:\n",
    "    samples.extend(construct_nli_dataset(file, 'human'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5990"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(samples) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test_df = pd.DataFrame(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "samples =[\n",
    "    'ad41a34db7e3975d6b83d2ddedb19d9f',\n",
    "    '857c595296caaeab532251cf9d8f3979',\n",
    "    'a41ba08ffb8af6eb5ecf70c7a52a6289'\n",
    "]\n",
    "test_df.loc[\n",
    "    test_df['sample'].isin(samples)\n",
    "].to_csv('../data/annotation_data/edit_consistency_pretest.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "longform_edit_model_evals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
