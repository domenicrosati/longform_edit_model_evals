{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from src.construct_samples import MAIN_PASSAGE_TEMPLATE, NEW_FACT_TEMPLATE, OLD_FACTS_RELATED_TEMPLATE, OLD_FACTS_SUBJECT_TEMPLATE, RELATED_PASSAGE_TEMPLATE, get_sample_text\n",
    "from src.utils import get_sample_id\n",
    "\n",
    "import os\n",
    "def get_samples_from_dir(dir_path):\n",
    "    samples = []\n",
    "    for file_name in os.listdir(dir_path):\n",
    "        with open(os.path.join(dir_path, file_name), 'r') as f:\n",
    "            samples.append(json.load(f))\n",
    "    return samples\n",
    "\n",
    "# Load the data\n",
    "methods = [\n",
    "    'gptj_ft_edit',\n",
    "    'gptj_ike_edit',\n",
    "    'gptj_rome_edit',\n",
    "    'gptj_no_edit',\n",
    "    'llama2_chat_ft_edit',\n",
    "    'llama2_chat_ike_edit',\n",
    "    'llama2_chat_rome_edit',\n",
    "    'llama2_chat_no_edit'\n",
    "]\n",
    "\n",
    "model = 'gpt-3.5-turbo-0613'\n",
    "analysis_data = {\n",
    "    k: json.load(open(f'../results/broken_out_survey_{k}_{model}.json')) for k in methods\n",
    "}\n",
    "samples = json.load(open('../data/counterfact_with_dependancies_sample.json'))\n",
    "\n",
    "# model_gptj_no_edit_False_use_sampling_True_token_length_1024_method_FT\n",
    "sample_id_map = {\n",
    "    \"gptj_ft_edit\": {\n",
    "    get_sample_id(s): s for s in\n",
    "    get_samples_from_dir(\"../data/generated_samples/model_gptj_no_edit_False_use_sampling_True_token_length_1024_method_FT\")\n",
    "    },\n",
    "    \"gptj_rome_edit\": {\n",
    "    get_sample_id(s): s for s in\n",
    "    get_samples_from_dir(\"../data/generated_samples/model_gptj_no_edit_False_use_sampling_True_token_length_1024_method_ROME\")\n",
    "    },\n",
    "    \"gptj_ike_edit\": {\n",
    "    get_sample_id(s): s for s in\n",
    "    get_samples_from_dir(\"../data/generated_samples/model_gptj_no_edit_False_use_sampling_True_token_length_1024_method_IKE\")\n",
    "    },\n",
    "    \"gptj_no_edit\": {\n",
    "    get_sample_id(s): s for s in\n",
    "    get_samples_from_dir(\"../data/generated_samples/model_gptj_no_edit_TRUE_use_sampling_True_token_length_1024_method_ROME\")\n",
    "    },\n",
    "    \"llama2_chat_ft_edit\": {\n",
    "    get_sample_id(s): s for s in\n",
    "    get_samples_from_dir(\"../data/generated_samples/model_llama2-chat_no_edit_False_use_sampling_True_token_length_1024_method_FT\")\n",
    "    },\n",
    "    \"llama2_chat_rome_edit\": {\n",
    "    get_sample_id(s): s for s in\n",
    "    get_samples_from_dir(\"../data/generated_samples/model_llama2-chat_no_edit_False_use_sampling_True_token_length_1024_method_ROME\")\n",
    "    },\n",
    "    \"llama2_chat_ike_edit\": {\n",
    "    get_sample_id(s): s for s in\n",
    "    get_samples_from_dir(\"../data/generated_samples/model_llama2-chat_no_edit_False_use_sampling_True_token_length_1024_method_IKE\")\n",
    "    },\n",
    "    \"llama2_chat_no_edit\": {\n",
    "    get_sample_id(s): s for s in\n",
    "    get_samples_from_dir(\"../data/generated_samples/model_llama2-chat_no_edit_TRUE_use_sampling_True_token_length_1024_method_ROME\")\n",
    "    },\n",
    "}\n",
    "\n",
    "SURVEY_ITEM_TO_SAMPLES_TEMPLATE = {\n",
    "    \"new_fact_main_passage\": [\n",
    "        NEW_FACT_TEMPLATE,\n",
    "        MAIN_PASSAGE_TEMPLATE,\n",
    "    ],\n",
    "    \"new_fact_related_passage\": [\n",
    "        NEW_FACT_TEMPLATE,\n",
    "        RELATED_PASSAGE_TEMPLATE,\n",
    "    ],\n",
    "    \"main_passage_old_facts\": [\n",
    "        MAIN_PASSAGE_TEMPLATE,\n",
    "        OLD_FACTS_SUBJECT_TEMPLATE,\n",
    "    ],\n",
    "    \"related_passage_old_facts\": [\n",
    "        RELATED_PASSAGE_TEMPLATE,\n",
    "        OLD_FACTS_RELATED_TEMPLATE,\n",
    "    ],\n",
    "    \"main_passage_consistency\": [\n",
    "        MAIN_PASSAGE_TEMPLATE,\n",
    "    ],\n",
    "    \"related_passage_consistency\": [\n",
    "        RELATED_PASSAGE_TEMPLATE,\n",
    "    ],\n",
    "    \"cross_passage_consistency\": [\n",
    "        MAIN_PASSAGE_TEMPLATE,\n",
    "        RELATED_PASSAGE_TEMPLATE,\n",
    "    ],\n",
    "    \"topicality\": [\n",
    "        MAIN_PASSAGE_TEMPLATE,\n",
    "        RELATED_PASSAGE_TEMPLATE,\n",
    "    ],\n",
    "    \"fluency\": [\n",
    "        MAIN_PASSAGE_TEMPLATE,\n",
    "        RELATED_PASSAGE_TEMPLATE,\n",
    "    ]\n",
    "}\n",
    "human_df = pd.read_csv('../results/human_survey_responses.csv')\n",
    "human_df['split'] = 'human'\n",
    "human_rated_samples = {\n",
    "    \"human\": {\n",
    "        get_sample_id(s): s for s in\n",
    "        get_samples_from_dir(\"../data/survey_samples/human\")\n",
    "    },  \n",
    "    \"rome\": {\n",
    "        get_sample_id(s): s for s in\n",
    "        get_samples_from_dir(\"../data/survey_samples/rome\")\n",
    "    },\n",
    "    \"no_edit\": {\n",
    "        get_sample_id(s): s for s in\n",
    "        get_samples_from_dir(\"../data/survey_samples/no_edit\")\n",
    "    },\n",
    "}\n",
    "human_samples = []\n",
    "question_to_label = {\n",
    "    'The main passage is written as if the new fact is true': 'new_fact_main_passage',\n",
    "    'The related passage does not contradict the new fact': 'new_fact_related_passage',\n",
    "    'Ignoring the new fact, most of the old facts are still true in the main passage.': 'main_passage_old_facts',\n",
    "    'Ignoring the new fact, most of the old facts are still true in the related passage.': 'related_passage_old_facts',\n",
    "    'Ignoring the old and new facts, the main passage does not contradict itself.': 'main_passage_consistency',\n",
    "    'Ignoring the old and new facts, the related passage does not contradict itself.': 'related_passage_consistency',\n",
    "    'Ignoring the old and new facts, the main passage and the related passage do not contradict each other.': 'cross_passage_consistency',\n",
    "    'The main passage is focused on the subject and the related passage is focused on the related entity': 'topicality',\n",
    "    'Both passages are natural sounding text close to what a human would write.': 'fluency'\n",
    "}\n",
    "for i, human_sample in human_df.iterrows():\n",
    "    sample_id = human_sample['sample_id']\n",
    "    method = human_sample['method']\n",
    "    sample = human_rated_samples[method][sample_id]\n",
    "    label = question_to_label[human_sample['question']]\n",
    "    human_samples.append({\n",
    "        'label': label,\n",
    "        'score': human_sample['response'],\n",
    "        'content': get_sample_text(\n",
    "            sample,\n",
    "            templates_to_use=SURVEY_ITEM_TO_SAMPLES_TEMPLATE[label]\n",
    "        ),\n",
    "        'intervention': method,\n",
    "        'model': 'human',\n",
    "        'sample_id': sample_id,\n",
    "        'split': 'human'\n",
    "    })\n",
    "\n",
    "human_df = pd.DataFrame(human_samples).fillna(4)\n",
    "\n",
    "survey_dfs = []\n",
    "for method, sample in analysis_data.items():\n",
    "    base_model = method.split('_')[0]\n",
    "    for sample_id, data in sample.items():\n",
    "        for label, scores in data.items():\n",
    "            templates_to_use = SURVEY_ITEM_TO_SAMPLES_TEMPLATE[label]\n",
    "            for score in scores:\n",
    "                survey_dfs.append({\n",
    "                    'label': label,\n",
    "                    'score': score,\n",
    "                    'content': get_sample_text(\n",
    "                        sample_id_map[method][sample_id],\n",
    "                        templates_to_use=templates_to_use\n",
    "                    ),\n",
    "                    'intervention': method,\n",
    "                    'model': base_model,\n",
    "                    'sample_id': sample_id,\n",
    "                    'split': 'generated'\n",
    "                })\n",
    "\n",
    "\n",
    "survey_df = pd.DataFrame(human_samples + survey_dfs).fillna(4)\n",
    "survey_df.to_csv('../results/survey_ratings_dataset.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = \"../data/annotation_data/longform_eval_final_results_annotations.json\"\n",
    "pretest = \"../data/annotation_data/longform_eval_first_3_samples_paragraph_annotations (2).json\"\n",
    "\n",
    "final = json.load(open(final))\n",
    "pretest = json.load(open(pretest))\n",
    "\n",
    "pretest_df = []\n",
    "import json\n",
    "import re\n",
    "with open('../data/annotation_data/longform_eval_first_3_samples_paragraph_annotations (2).json') as f:\n",
    "    pretest = json.load(f)\n",
    "\n",
    "intervention_map = {\n",
    "    'human': 'llama2_chat_human_edit_pretest_annotation',\n",
    "    'no_edit': 'llama2_chat_no_edit_pretest_annotation',\n",
    "    'rome': 'llama2_chat_rome_edit_pretest_annotation'\n",
    "}\n",
    "\n",
    "anno_question_type = {\n",
    "    'new_fact_and_main_passage': 'New fact is true',\n",
    "    'new_fact_and_related_passage': 'New fact is true',\n",
    "    'old_fact_and_main_passage': 'Old fact is true',\n",
    "    'old_fact_and_related_passage': 'Old fact is true',\n",
    "    'ground_truth_and_related_passage': 'Ground truth is true',\n",
    "    'ground_truth_and_main_passage': 'Ground truth is true',\n",
    "}\n",
    "\n",
    "\n",
    "rating_to_number = {\n",
    "    'supports': 2,\n",
    "    'neutral': 1,\n",
    "    'contradicts': 0\n",
    "}\n",
    "\n",
    "ratings = []\n",
    "for example in pretest['examples']:\n",
    "    # Regex to get the Claim: part from content\n",
    "    claim = re.search('Claim: (.*)', example['content']).group(1)\n",
    "    for annotation in example['annotations']:\n",
    "        # value, tag\n",
    "        ratings.append({\n",
    "            \"content\": f\"\"\"\n",
    "Passage: {annotation['value']}\n",
    "Claim: {claim}\n",
    "            \"\"\".strip(),\n",
    "            \"sample_id\": example['metadata']['sample'],\n",
    "            \"example_id\": example['example_id'],\n",
    "            \"classification\": annotation['tag'],\n",
    "            \"method\": example['metadata']['intervention'],\n",
    "            \"label\": example['metadata']['label'],\n",
    "            'question_type': anno_question_type[\n",
    "                example['metadata']['label']\n",
    "            ],\n",
    "        })\n",
    "\n",
    "    for rating in example['classifications']:\n",
    "        for rater in rating['classified_by']:\n",
    "            anon_id = rater['annotator_id']\n",
    "\n",
    "            ratings.append({\n",
    "                \"content\": example['content'],\n",
    "                \"sample_id\": example['metadata']['sample'],\n",
    "                \"example_id\": example['example_id'],\n",
    "                \"classification\": rating['classname'],\n",
    "                \"method\": example['metadata']['intervention'],\n",
    "                \"label\": example['metadata']['label'],\n",
    "                'question_type': anno_question_type[\n",
    "                    example['metadata']['label']\n",
    "                ],\n",
    "            })\n",
    "\n",
    "for example in final['examples']:\n",
    "    # Regex to get the Claim: part from content\n",
    "    claim = re.search('Claim: (.*)', example['content']).group(1)\n",
    "    for annotation in example['annotations']:\n",
    "        # value, tag\n",
    "        ratings.append({\n",
    "            \"content\": f\"\"\"\n",
    "Passage: {annotation['value']}\n",
    "Claim: {claim}\n",
    "            \"\"\".strip(),\n",
    "            \"sample_id\": example['metadata']['sample'],\n",
    "            \"example_id\": example['example_id'],\n",
    "            \"classification\": annotation['tag'],\n",
    "            \"method\": example['metadata']['intervention'],\n",
    "            \"label\": example['metadata']['label'],\n",
    "            'question_type': anno_question_type[\n",
    "                example['metadata']['label']\n",
    "            ],\n",
    "        })\n",
    "\n",
    "    for rating in example['classifications']:\n",
    "        for rater in rating['classified_by']:\n",
    "            anon_id = rater['annotator_id']\n",
    "\n",
    "            ratings.append({\n",
    "                \"content\": example['content'],\n",
    "                \"sample_id\": example['metadata']['sample'],\n",
    "                \"example_id\": example['example_id'],\n",
    "                \"classification\": rating['classname'],\n",
    "                \"method\": example['metadata']['intervention'],\n",
    "                \"label\": example['metadata']['label'],\n",
    "                'question_type': anno_question_type[\n",
    "                    example['metadata']['label']\n",
    "                ],\n",
    "            })\n",
    "            \n",
    "\n",
    "annos_df = pd.DataFrame(ratings)\n",
    "annos_df.drop_duplicates(subset=[\"content\"], inplace=True)\n",
    "annos_df.to_csv('../results/annotations_dataset.csv', index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
