{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open('../data/annotation_data/longform_eval_first_3_samples_paragraph_annotations (2).json') as f:\n",
    "    pretest = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "425"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raters = {\n",
    "\n",
    "}\n",
    "rating_to_number = {\n",
    "    'supports': 0,\n",
    "    'neutral': 1,\n",
    "    'contradicts': 2\n",
    "}\n",
    "agreements = {\n",
    "    1: 0,\n",
    "    2: 0,\n",
    "    3: 0,\n",
    "    4: 0\n",
    "}\n",
    "agreements_by_class = {\n",
    "    'supports': {\n",
    "        1: 0,\n",
    "        2: 0,\n",
    "        3: 0,\n",
    "        4: 0\n",
    "    },\n",
    "    'neutral': {\n",
    "        1: 0,\n",
    "        2: 0,\n",
    "        3: 0,\n",
    "        4: 0\n",
    "    },\n",
    "    'contradicts': {\n",
    "        1: 0,\n",
    "        2: 0,\n",
    "        3: 0,\n",
    "        4: 0\n",
    "    }\n",
    "}\n",
    "annos = []\n",
    "for example in pretest['examples']:\n",
    "    agreements[\n",
    "        len(example['classifications'])\n",
    "    ] += 1\n",
    "    for rating in example['classifications']:\n",
    "        for rater in rating['classified_by']:\n",
    "            anon_id = rater['annotator_id']\n",
    "            if anon_id not in raters:\n",
    "                raters[anon_id] = []\n",
    "            raters[anon_id].append(rating_to_number[rating['classname']])\n",
    "            agreements_by_class[rating['classname']][len(example['classifications'])] += 1\n",
    "            annos.append({\n",
    "                'anno_id': anon_id,\n",
    "                'sample_id':example['metadata']['sample'],\n",
    "                'example_id': example['example_id'],\n",
    "                'rating': rating['classname'],\n",
    "                'rating_num': rating_to_number[rating['classname']],\n",
    "            })\n",
    "anno_df = pd.DataFrame(annos)\n",
    "len(anno_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get example_id: rating for each annotator\n",
    "two_samples = []\n",
    "three_samples = []\n",
    "four_samples = []\n",
    "for example_id in anno_df.example_id.unique():\n",
    "    sample_ratings = anno_df[anno_df.example_id == example_id]\n",
    "    if len(sample_ratings) == 2:\n",
    "        two_samples.append(sample_ratings['rating_num'].values)\n",
    "    elif len(sample_ratings) == 3:\n",
    "        three_samples.append(sample_ratings['rating_num'].values)\n",
    "        two_samples.append(sample_ratings['rating_num'].values[:2])\n",
    "        two_samples.append(sample_ratings['rating_num'].values[1:3])\n",
    "    elif len(sample_ratings) == 4:\n",
    "        four_samples.append(sample_ratings['rating_num'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(239, 54, 0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(two_samples), len(three_samples), len(four_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5683305732986358"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import krippendorff as kd\n",
    "import numpy as np\n",
    "kd.alpha(\n",
    "    np.array(three_samples).T,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three samples, 1 and 2 0.7339901477832512\n",
      "Three samples, 0 and 1 0.6324446965399886\n",
      "Three samples, 0 and 2 0.43925233644859807\n",
      "Two samples, 0 and 1 0.6462365001918755\n"
     ]
    }
   ],
   "source": [
    "# cohens kappa\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "print(\"Three samples, 1 and 2\", cohen_kappa_score(\n",
    "    np.array(three_samples).T[1],\n",
    "    np.array(three_samples).T[2]\n",
    "))\n",
    "print(\"Three samples, 0 and 1\", cohen_kappa_score(\n",
    "    np.array(three_samples).T[0],\n",
    "    np.array(three_samples).T[1]\n",
    "))\n",
    "print(\"Three samples, 0 and 2\", cohen_kappa_score(\n",
    "    np.array(three_samples).T[0],\n",
    "    np.array(three_samples).T[2]\n",
    "))\n",
    "print(\"Two samples, 0 and 1\", cohen_kappa_score(\n",
    "    np.array(two_samples).T[0],\n",
    "    np.array(two_samples).T[1]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.array(three_samples).T[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6382441977800202"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statsmodels.stats import inter_rater as irr\n",
    "agg = irr.aggregate_raters(\n",
    "    np.array(two_samples) \n",
    ")\n",
    "irr.fleiss_kappa(agg[0], method='fleiss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "gpt_35_annotations_human_path = '../results/annotations_llama2_chat_human_edit_pretest_annotation_gpt-3.5-turbo-0613.json'\n",
    "gpt_35_annotations_human = json.load(open(gpt_35_annotations_human_path))\n",
    "\n",
    "gpt_35_annotations_no_edit_path = '../results/annotations_llama2_chat_no_edit_pretest_annotation_gpt-3.5-turbo-0613.json'\n",
    "gpt_35_annotations_no_edit = json.load(open(gpt_35_annotations_no_edit_path))\n",
    "\n",
    "gpt_35_annotations_edit_path = '../results/annotations_llama2_chat_rome_edit_pretest_annotation_gpt-3.5-turbo-0613.json'\n",
    "gpt_35_annotations_edit = json.load(open(gpt_35_annotations_edit_path))\n",
    "\n",
    "gpt_35_df = pd.DataFrame(\n",
    "    gpt_35_annotations_human + gpt_35_annotations_no_edit + gpt_35_annotations_edit\n",
    ")\n",
    "\n",
    "gpt_4_annotations_human_path = '../results/annotations_llama2_chat_human_edit_pretest_annotation_gpt-4.json'\n",
    "gpt_4_annotations_human = json.load(open(gpt_4_annotations_human_path))\n",
    "\n",
    "gpt_4_annotations_no_edit_path = '../results/annotations_llama2_chat_no_edit_pretest_annotation_gpt-4.json'\n",
    "gpt_4_annotations_no_edit = json.load(open(gpt_4_annotations_no_edit_path))\n",
    "\n",
    "gpt_4_annotations_edit_path = '../results/annotations_llama2_chat_rome_edit_pretest_annotation_gpt-4.json'\n",
    "gpt_4_annotations_edit = json.load(open(gpt_4_annotations_edit_path))\n",
    "\n",
    "gpt_4_df = pd.DataFrame(\n",
    "    gpt_4_annotations_human + gpt_4_annotations_no_edit + gpt_4_annotations_edit\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../data/annotation_data/longform_eval_first_3_samples_paragraph_annotations.json') as f:\n",
    "    pretest = json.load(f)\n",
    "\n",
    "intervention_map = {\n",
    "    'human': 'llama2_chat_human_edit_pretest_annotation',\n",
    "    'no_edit': 'llama2_chat_no_edit_pretest_annotation',\n",
    "    'rome': 'llama2_chat_rome_edit_pretest_annotation'\n",
    "}\n",
    "\n",
    "raters = {\n",
    "    9: [],\n",
    "    10: []\n",
    "}\n",
    "rating_to_number = {\n",
    "    'supports': 0,\n",
    "    'neutral': 1,\n",
    "    'contradicts': 2\n",
    "}\n",
    "agreements = {\n",
    "    1: 0,\n",
    "    2: 0,\n",
    "    3: 0,\n",
    "    4: 0\n",
    "}\n",
    "agreements_by_class = {\n",
    "    'contradicts': {\n",
    "        1: 0,\n",
    "        2: 0,\n",
    "        3: 0,\n",
    "        4: 0\n",
    "    },\n",
    "    'neutral': {\n",
    "        1: 0,\n",
    "        2: 0,\n",
    "        3: 0,\n",
    "        4: 0\n",
    "    },\n",
    "    'supports': {\n",
    "        1: 0,\n",
    "        2: 0,\n",
    "        3: 0,\n",
    "        4: 0\n",
    "    }\n",
    "}\n",
    "for example in pretest['examples']:\n",
    "    agreements[\n",
    "        len(example['classifications'])\n",
    "    ] += 1\n",
    "    for rating in example['classifications']:\n",
    "        for rater in rating['classified_by']:\n",
    "            anon_id = rater['annotator_id']\n",
    "            if anon_id not in raters:\n",
    "                raters[anon_id] = []\n",
    "            \n",
    "            agreements_by_class[rating['classname']][len(example['classifications'])] += 1\n",
    "            raters[9].append(\n",
    "                gpt_35_df.loc[\n",
    "                    (gpt_35_df['content'] == example['content'])\n",
    "                ]['classification'].values[0]\n",
    "            )\n",
    "            raters[10].append(\n",
    "                gpt_4_df.loc[\n",
    "                    (gpt_4_df['content'] == example['content'])\n",
    "                ]['classification'].values[0]\n",
    "            )\n",
    "            raters[anon_id].append(rating['classname'])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/domenicrosati/src/longform_edit_model_evals/Notebooks/annotations_analysis.ipynb Cell 9\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/domenicrosati/src/longform_edit_model_evals/Notebooks/annotations_analysis.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# cohens kappa\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/domenicrosati/src/longform_edit_model_evals/Notebooks/annotations_analysis.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m cohen_kappa_score\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/domenicrosati/src/longform_edit_model_evals/Notebooks/annotations_analysis.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m cohen_kappa_score(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/domenicrosati/src/longform_edit_model_evals/Notebooks/annotations_analysis.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     raters[\u001b[39m10\u001b[39m],\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/domenicrosati/src/longform_edit_model_evals/Notebooks/annotations_analysis.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     raters[\u001b[39m5\u001b[39;49m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/domenicrosati/src/longform_edit_model_evals/Notebooks/annotations_analysis.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m )\n",
      "\u001b[0;31mKeyError\u001b[0m: 5"
     ]
    }
   ],
   "source": [
    "# cohens kappa\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "cohen_kappa_score(\n",
    "    raters[10],\n",
    "    raters[5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "longform_edit_model_evals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
