{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "DATASET_PATH = '../data/counterfact.json'\n",
    "with open(DATASET_PATH, 'r') as f:\n",
    "    dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21919"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwikidata.entity import WikidataItem, WikidataProperty\n",
    "import qwikidata.linked_data_interface as ldi\n",
    "from qwikidata.sparql import return_sparql_query_results\n",
    "import functools\n",
    "\n",
    "ent_id2label ={}\n",
    "with open('../data/ent_id2label.json', 'r') as f:\n",
    "    ent_id2label = json.load(f)\n",
    "\n",
    "LABEL_TO_QID = \"\"\"\n",
    "PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "SELECT DISTINCT ?item\n",
    "WHERE {{\n",
    "\n",
    "  # make input string into a language-tagged string\n",
    "  BIND( STRLANG(\"{}\", \"en\") AS ?label ) .\n",
    "\n",
    "  # search all items that have this languaged-tagged string as label\n",
    "  ?item rdfs:label ?label .\n",
    "\n",
    "  # extract the last path segment of the URI\n",
    "  BIND(STRAFTER(STR(?item), STR(wd:)) AS ?qid) .\n",
    "  # get count of propertes as heuristic for popularity\n",
    "    ?item ?p ?statement .\n",
    "}} order by desc(?p) limit 1\n",
    "\"\"\"\n",
    "\n",
    "ALL_ENTITIES_THAT_HAVE_THIS_AS_AN_OBJECT_TO_ANY_PROPERTY = \"\"\"\n",
    "SELECT DISTINCT ?item\n",
    "WHERE {{\n",
    "  ?item ?p wd:{0} .\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "ALL_ENTITIES_THAT_HAVE_THIS_AS_AN_OBJECT_TO_ANY_PROPERTY_AND_THIS_HAS = \"\"\"\n",
    "SELECT DISTINCT ?item\n",
    "WHERE {{\n",
    "  ?item ?p wd:{0} .\n",
    "  wd:{1} ?p2 ?item. # change to ?p for mutual property\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "ALL_ENTITIES_THAT_HAVE_THIS_AS_AN_OBJECT_TO_ANY_PROPERTY_AND_PRE_EDIT = \"\"\"\n",
    "SELECT DISTINCT ?item\n",
    "WHERE {{\n",
    "  ?item ?p wd:{0} .\n",
    "  ?item ?p2 wd:{1} .\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "COUPLED_ENTITIES_QUERY = \"\"\"\n",
    "SELECT ?item\n",
    "WHERE {{\n",
    "  {{ ?item ?p wd:{0} . }}\n",
    "  UNION\n",
    "  {{ wd:{0} ?p ?item . }}\n",
    "  ?item ?p2 wd:{1} .\n",
    "}} GROUP BY ?item ORDER BY DESC(COUNT(?item)) LIMIT 5\n",
    "\"\"\"\n",
    "\n",
    "@functools.lru_cache()\n",
    "def get_wikidata_item_from_label(label):\n",
    "    try:\n",
    "      qid = return_sparql_query_results(LABEL_TO_QID.format(label))['results']['bindings'][0]['item']['value']\n",
    "    except Exception as e:\n",
    "      print(e)\n",
    "      return None\n",
    "    entity_dict = ldi.get_entity_dict_from_api(qid.strip('http://www.wikidata.org/entity/'))\n",
    "    return WikidataItem(entity_dict)\n",
    "\n",
    "@functools.lru_cache()\n",
    "def get_wikidata_item_from_id(qid):\n",
    "    try:\n",
    "       entity_dict = ldi.get_entity_dict_from_api(qid)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    return WikidataItem(entity_dict)\n",
    "\n",
    "@functools.lru_cache()\n",
    "def get_wikidata_label_from_id(qid):\n",
    "    try:\n",
    "      entity_label = ent_id2label[qid]\n",
    "      return entity_label\n",
    "    except Exception as e:\n",
    "      entity = get_wikidata_item_from_id(qid)\n",
    "      if entity:\n",
    "        return entity.get_label()\n",
    "      else:\n",
    "        return ''\n",
    "\n",
    "@functools.lru_cache()\n",
    "def get_wikidata_property_from_id(pid):\n",
    "    try:\n",
    "      entity_dict = ldi.get_entity_dict_from_api(pid)\n",
    "    except Exception as e:\n",
    "      print(e)\n",
    "      return None\n",
    "    return WikidataProperty(entity_dict)\n",
    "\n",
    "@functools.lru_cache()\n",
    "def get_all_entities_that_have_this_as_an_object_to_any_property(qid):\n",
    "    try:\n",
    "      items = return_sparql_query_results(ALL_ENTITIES_THAT_HAVE_THIS_AS_AN_OBJECT_TO_ANY_PROPERTY.format(qid, qid))['results']['bindings']\n",
    "    except Exception as e:\n",
    "      print(e)\n",
    "      return []\n",
    "    entities = []\n",
    " \n",
    "    for item in items:\n",
    "      if 'http://www.wikidata.org/entity/Q' in item['item']['value']:\n",
    "        entities.append(get_wikidata_item_from_id(item['item']['value'].strip('http://www.wikidata.org/entity/')))\n",
    "    return entities\n",
    "\n",
    "@functools.lru_cache()\n",
    "def get_coupled_entities(qid_entity, qid_change):\n",
    "    try:\n",
    "      items = return_sparql_query_results(COUPLED_ENTITIES_QUERY.format(qid_entity, qid_change))['results']['bindings']\n",
    "    except Exception as e:\n",
    "      print(e)\n",
    "      return []\n",
    "\n",
    "    entities = []\n",
    "    for item in items:\n",
    "      if 'http://www.wikidata.org/entity/Q' in item['item']['value']:\n",
    "        entities.append(get_wikidata_item_from_id(item['item']['value'].strip('http://www.wikidata.org/entity/')))\n",
    "    return entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: getting the property label is slow\n",
    "# getting the ID is preferrable\n",
    "property_exclude_list = [\n",
    "    'instance of',\n",
    "    'copyright status',\n",
    "    'described by source',\n",
    "    'documentation files at',\n",
    "    'category',\n",
    "    'copyright representative',\n",
    "    'modified version of',\n",
    "    'topic',\n",
    "    'main regulatory text',\n",
    "    'open data portal',\n",
    "    'rating',\n",
    "    'follow',\n",
    "    'said to be the same as',\n",
    "    'twinned',\n",
    "    'copyright license',\n",
    "    'list',\n",
    "    'access',\n",
    "    'wiki',\n",
    "    'duplicated',\n",
    "    'facet of',\n",
    "    'translation',\n",
    "    'via',\n",
    "    'classification'\n",
    "]\n",
    "\n",
    "def _check_if_in_exclude_list(property_label):\n",
    "    for exclude_word in property_exclude_list:\n",
    "        if exclude_word in property_label.lower():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def _construct_ground_truth(entity_property_map, property_id_label_map):\n",
    "    ground_truth = {}\n",
    "    for property_id, property_label in property_id_label_map.items():\n",
    "        if property_id in entity_property_map:\n",
    "            ground_truth[property_label] = [\n",
    "                get_wikidata_label_from_id(qid)\n",
    "                for qid in entity_property_map[property_id]\n",
    "            ]\n",
    "    return ground_truth\n",
    "\n",
    "def get_overlap(property_label_map_1, property_label_map_2):\n",
    "    # get overlap between two property_label_maps \n",
    "    # i.e. the number of key value pairs that are the same\n",
    "    overlap = []\n",
    "    for property, labels in property_label_map_2.items():\n",
    "        if property in property_label_map_1 and len(set(property_label_map_1[property]) & set(labels)) > 0:\n",
    "            overlap.append((property, property_label_map_2[property]))\n",
    "    return overlap\n",
    "\n",
    "def _get_coupled_properties_count(  \n",
    "    ent_property_label_map,\n",
    "    property_label_map,\n",
    "    target_true,\n",
    "    ent_id,\n",
    "    qid\n",
    "):\n",
    "    coupled_properties = 0\n",
    "    for property, labels in property_label_map.items():\n",
    "        # subject is object of this entity\n",
    "        if ent_id in labels:\n",
    "            coupled_properties += 1\n",
    "        # target true is object of this entity\n",
    "        if target_true in labels:\n",
    "            coupled_properties += 1\n",
    "        # mutual property\n",
    "        if ent_id in labels and property in ent_property_label_map and qid in ent_property_label_map[property]:\n",
    "            coupled_properties += 1\n",
    "    return coupled_properties\n",
    "\n",
    "\n",
    "def construct_entity_properties(\n",
    "    original_entity, \n",
    "    related_entity_dict, \n",
    "    ent_property_label_map,\n",
    "    original_propery,\n",
    "    target_true,\n",
    "    requested_rewrite\n",
    "):\n",
    "    original_propery_label = get_wikidata_property_from_id(original_propery).get_label()\n",
    "    coupled_entities = []\n",
    "    \n",
    "    property_id_label_map = {}\n",
    "    mutual_properties = set()\n",
    "    subject_as_object = set()\n",
    "    target_true_as_object = set()\n",
    "    original_property_of_subject_as_object = set()\n",
    "    related_entity_label = related_entity_dict['entity'].get_label()\n",
    "\n",
    "    dependant_prompt = f\"\"\"Write an essay about {related_entity_label}\n",
    "Include the following information:\"\"\"\n",
    "    original_property_of_subject_as_object.add(original_propery_label)\n",
    "    for property, labels in related_entity_dict['property_label_map'].items():\n",
    "        # get interesting properties\n",
    "        if original_entity.entity_id in labels:\n",
    "            property_label = get_wikidata_property_from_id(property).get_label()\n",
    "            if _check_if_in_exclude_list(property_label):\n",
    "                continue\n",
    "            original_property_of_subject_as_object.add(f\"{original_propery_label} of {property_label}\")\n",
    "            subject_as_object.add(property_label)\n",
    "            property_id_label_map[property] = property_label\n",
    "        if target_true in labels:\n",
    "            property_label = get_wikidata_property_from_id(property).get_label()\n",
    "            if _check_if_in_exclude_list(property_label):\n",
    "                continue\n",
    "            target_true_as_object.add(property_label)\n",
    "            property_id_label_map[property] = property_label\n",
    "        if original_entity.entity_id in labels and property in ent_property_label_map and related_entity_dict['entity'].entity_id in ent_property_label_map[property]:\n",
    "            property_label = get_wikidata_property_from_id(property).get_label()\n",
    "            if _check_if_in_exclude_list(property_label):\n",
    "                continue\n",
    "            mutual_properties.add(property_label)\n",
    "            property_id_label_map[property] = property_label\n",
    "    \n",
    "    # if there are additional overlap properties add them\n",
    "    overlap_properties = set()\n",
    "    for property, _ in related_entity_dict['overlap']:\n",
    "        # get property label\n",
    "        # make properties labels a deduped list\n",
    "        property_label = get_wikidata_property_from_id(property).get_label()\n",
    "        if _check_if_in_exclude_list(property_label):\n",
    "            continue\n",
    "        overlap_properties.add(property_label)\n",
    "        property_id_label_map[property] = property_label\n",
    "\n",
    "    properties_added = set()\n",
    "    for property_label in original_property_of_subject_as_object:\n",
    "        if property_label in properties_added:\n",
    "            continue\n",
    "        dependant_prompt += f\"\\n- {property_label}\"\n",
    "        properties_added.add(property_label)\n",
    "    for property_label in mutual_properties:\n",
    "        if property_label in properties_added:\n",
    "            continue\n",
    "        dependant_prompt += f\"\\n- {property_label}\"\n",
    "        properties_added.add(property_label)\n",
    "    for property_label in target_true_as_object:\n",
    "        if property_label in properties_added:\n",
    "            continue\n",
    "        dependant_prompt += f\"\\n- {property_label}\"\n",
    "        properties_added.add(property_label)\n",
    "    for property_label in subject_as_object:\n",
    "        if property_label in properties_added:\n",
    "            continue\n",
    "        dependant_prompt += f\"\\n- {property_label}\"\n",
    "        properties_added.add(property_label)\n",
    "    for property_label in overlap_properties:\n",
    "        if property_label in properties_added:\n",
    "            continue\n",
    "        dependant_prompt += f\"\\n- {property_label}\"\n",
    "        properties_added.add(property_label)\n",
    "\n",
    "    coupled_entities.append({\n",
    "        'entity': related_entity_label,\n",
    "        'dependant_prompt': dependant_prompt,\n",
    "        'dependant_prompt_with_relationship': dependant_prompt + f\"\\nRelationship to:\\n- {requested_rewrite['subject']}\\n- {requested_rewrite['target_true']['str']}\\n- {requested_rewrite['target_new']['str']}\",\n",
    "        'mutual_properties': list(mutual_properties),\n",
    "        'subject_as_object': list(subject_as_object),\n",
    "        'target_true_as_object': list(target_true_as_object),\n",
    "        'overlap_properties': list(overlap_properties),\n",
    "        'original_property_of_subject_as_object': list(original_property_of_subject_as_object),\n",
    "        'ground_truth': _construct_ground_truth(\n",
    "            related_entity_dict['property_label_map'],\n",
    "            property_id_label_map\n",
    "        )\n",
    "    })\n",
    "    \n",
    "    # create story prompt for original entity\n",
    "    dependant_prompt = f\"\"\"Write an essay about {original_entity.get_label()}\n",
    "Include the following information:\"\"\"\n",
    "    dependant_prompt += f\"\\n- {original_propery_label}\"\n",
    "    \n",
    "    # TODO: add relationships here + guideprompts, it might be better here\n",
    "    subject_properties = set()\n",
    "    property_id_label_map = {}\n",
    "    for property, _ in ent_property_label_map.items():\n",
    "        # make properties a deduped list\n",
    "        # get property label\n",
    "        property_label = get_wikidata_property_from_id(property).get_label()\n",
    "        \n",
    "        if _check_if_in_exclude_list(property_label):\n",
    "            continue\n",
    "        subject_properties.add(property_label)\n",
    "        property_id_label_map[property] = property_label\n",
    "\n",
    "    for property_label in subject_properties:\n",
    "        dependant_prompt += f\"\\n- {property_label}\"\n",
    "    \n",
    "    subject_entity = {\n",
    "        'properties': list(subject_properties),\n",
    "        'dependant_prompt': dependant_prompt,\n",
    "        'dependant_prompt_with_relationship': dependant_prompt + f\"\\nRelationship to:\\n- {related_entity_label}\\n- {requested_rewrite['target_true']['str']}\\n- {requested_rewrite['target_new']['str']}\",\n",
    "        'ground_truth': _construct_ground_truth(\n",
    "            ent_property_label_map,\n",
    "            property_id_label_map\n",
    "        )\n",
    "    }\n",
    "    # sort coupled entities by number of properties\n",
    "    return {\n",
    "        'subject_entity': subject_entity,\n",
    "        'coupled_entities': coupled_entities\n",
    "    }\n",
    "\n",
    "def construct_dataset_item(\n",
    "        subject, \n",
    "        property_edited, \n",
    "        target_true,\n",
    "        requested_rewrite\n",
    "    ):\n",
    "    ent = get_wikidata_item_from_label(subject)\n",
    "    if ent is None:\n",
    "        return None\n",
    "    items = get_coupled_entities(ent.entity_id, target_true)\n",
    "    if len(items) == 0:\n",
    "        return None\n",
    "    # filter out items with the same label\n",
    "    items = list({\n",
    "        item.get_label(): item\n",
    "        for item in items\n",
    "        if item and item.get_label() and item.get_label() != ent.get_label()\n",
    "    }.values())\n",
    "    # get all claims for this item\n",
    "    claims = ent.get_truthy_claim_groups()\n",
    "    # for all claims get entity values\n",
    "    ent_property_label_map = {}\n",
    "    for property, claim_group in claims.items():\n",
    "        for claim in claim_group:\n",
    "            if 'WikibaseEntityId' in str(type(claim.mainsnak.datavalue)):\n",
    "                # TODO: ignore properties at this level (ids) for speed\n",
    "                if property not in ent_property_label_map:\n",
    "                    ent_property_label_map[property] = []\n",
    "                ent_property_label_map[property].append(claim.mainsnak.datavalue.value['id'])\n",
    "                #print(get_wikidata_item_from_id(claim.mainsnak.datavalue.value['id']))\n",
    "\n",
    "    # construct a property_label_map for all entities in property_label_map.values()\n",
    "    property_label_maps = {}\n",
    "    for item in items:\n",
    "        qid = item.entity_id\n",
    "        property_label_maps[qid] = {}\n",
    "        claims = item.get_truthy_claim_groups()\n",
    "        for property, claim_group in claims.items():\n",
    "            for claim in claim_group:\n",
    "                if 'WikibaseEntityId' in str(type(claim.mainsnak.datavalue)):\n",
    "                    if property not in property_label_maps[qid]:\n",
    "                        property_label_maps[qid][property] = []\n",
    "                    # TODO: ignore properties at this level (ids) for speed\n",
    "                    property_label_maps[qid][property].append(claim.mainsnak.datavalue.value['id'])\n",
    "\n",
    "    \n",
    "    # rank property_label_maps by overlap with property_label_map\n",
    "    entity_with_overlap = {}\n",
    "    for qid, property_label_map in property_label_maps.items():\n",
    "        for labels in property_label_map.values():\n",
    "            if ent.entity_id in labels and qid not in entity_with_overlap:\n",
    "                if qid == ent.entity_id:\n",
    "                    continue\n",
    "                overlap = get_overlap(ent_property_label_map, property_label_map)\n",
    "                overlap_count = len(overlap)\n",
    "                coupled_properties_count = _get_coupled_properties_count(\n",
    "                    ent_property_label_map,\n",
    "                    property_label_map,\n",
    "                    target_true,\n",
    "                    ent.entity_id,\n",
    "                    qid\n",
    "                )\n",
    "                entity_with_overlap[qid] = {\n",
    "                    'overlap': overlap,\n",
    "                    'interesting_property_count': overlap_count + coupled_properties_count,\n",
    "                    'entity': get_wikidata_item_from_id(qid),\n",
    "                    'property_label_map': property_label_map\n",
    "                }\n",
    "\n",
    "    entity_with_overlap = list(entity_with_overlap.values())\n",
    "    entity_with_overlap.sort(key=lambda x: x['interesting_property_count'], reverse=True)\n",
    "    if len(entity_with_overlap) == 0:\n",
    "        return None\n",
    "\n",
    "    dependant_prompts = construct_entity_properties(\n",
    "        ent, entity_with_overlap[0],\n",
    "        ent_property_label_map, \n",
    "        property_edited, \n",
    "        target_true,\n",
    "        requested_rewrite\n",
    "    )\n",
    "    return  {\n",
    "        **dependant_prompts,\n",
    "        'coupled_properties_count': entity_with_overlap[0]['interesting_property_count'],\n",
    "    }\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/21919 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 191/21919 [13:01<229:48:52, 38.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 282/21919 [21:07<16:19:38,  2.72s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 330/21919 [24:38<49:22:59,  8.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_dict['type'] must be 'item' but found 'lexeme'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 354/21919 [26:09<13:59:40,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 414/21919 [31:40<22:31:30,  3.77s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 472/21919 [36:35<35:01:16,  5.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 474/21919 [36:38<21:59:58,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 557/21919 [43:25<50:58:23,  8.59s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_dict['type'] must be 'item' but found 'lexeme'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 607/21919 [47:46<31:28:31,  5.32s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 609/21919 [47:49<20:16:31,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 612/21919 [48:10<40:59:13,  6.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 659/21919 [50:59<18:00:33,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 758/21919 [59:08<25:25:18,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 804/21919 [1:03:55<30:06:17,  5.13s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 859/21919 [1:10:42<23:43:26,  4.06s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 892/21919 [1:15:26<30:54:58,  5.29s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 920/21919 [1:17:36<20:14:58,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 921/21919 [1:17:37<16:22:11,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 922/21919 [1:17:38<13:38:58,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 929/21919 [1:17:53<13:11:29,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 936/21919 [1:18:54<34:03:09,  5.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 1042/21919 [1:27:24<38:13:29,  6.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 1070/21919 [1:29:29<15:43:05,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 1079/21919 [1:30:08<29:46:16,  5.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 1083/21919 [1:30:17<16:59:50,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1172/21919 [1:37:46<60:59:50, 10.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1185/21919 [1:39:14<28:56:22,  5.02s/it]\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device: '../data/counterfact_with_dependancies.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/Users/domenicrosati/src/longform_edit_model_evals/Notebooks/produce_counterfact_gen_dataset.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/domenicrosati/src/longform_edit_model_evals/Notebooks/produce_counterfact_gen_dataset.ipynb#W4sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m items\u001b[39m.\u001b[39mappend(new_item)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/domenicrosati/src/longform_edit_model_evals/Notebooks/produce_counterfact_gen_dataset.ipynb#W4sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m# append new item to dataset file without overwriting\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/domenicrosati/src/longform_edit_model_evals/Notebooks/produce_counterfact_gen_dataset.ipynb#W4sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39m../data/counterfact_with_dependancies.json\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mw\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/domenicrosati/src/longform_edit_model_evals/Notebooks/produce_counterfact_gen_dataset.ipynb#W4sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     json\u001b[39m.\u001b[39mdump(items, f, indent\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/longform_edit_model_evals/lib/python3.9/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device: '../data/counterfact_with_dependancies.json'"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "items = []\n",
    "# np.random.seed(42)\n",
    "# sample_dataset = np.random.choice(dataset, 10000)\n",
    "for item in tqdm(dataset):\n",
    "    # twinned cities seems to hard\n",
    "    if item['requested_rewrite']['relation_id'] == 'P190':\n",
    "        continue\n",
    "    subject = item['requested_rewrite']['subject']\n",
    "    try:\n",
    "        dependant_prompts = construct_dataset_item(\n",
    "            subject, \n",
    "            item['requested_rewrite']['relation_id'],\n",
    "            item['requested_rewrite']['target_true']['id'],\n",
    "            item['requested_rewrite']\n",
    "        )\n",
    "        if not dependant_prompts:\n",
    "            time.sleep(1)\n",
    "            continue\n",
    "        if dependant_prompts['coupled_entities'] == []:\n",
    "            time.sleep(1)\n",
    "            continue\n",
    "    except Exception as e:\n",
    "        time.sleep(1)\n",
    "        print(e)\n",
    "        continue\n",
    "\n",
    "    new_item = ({\n",
    "        **item,\n",
    "        'dependancies':  dependant_prompts\n",
    "    })\n",
    "    items.append(new_item)\n",
    "    \n",
    "    # append new item to dataset file without overwriting\n",
    "    with open('../data/counterfact_with_dependancies.json', 'w') as f:\n",
    "        json.dump(items, f, indent=4)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "longform_edit_model_evals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
