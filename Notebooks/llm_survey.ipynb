{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUCTION_PROMPT = \"\"\"\n",
    "You are a particpant in a survey about natural language generation systems. Please enter the survey to the best of your ability. \n",
    "\n",
    "For each survey item: Answer a number between 1 and 7\n",
    "1 = Strongly Disagree\n",
    "2 = Disagree\n",
    "3 = Somewhat Disagree\n",
    "4 = Neither Agree nor Disagree\n",
    "5 = Somewhat Agree\n",
    "6 = Agree\n",
    "7 = Strongly Agree\n",
    "\n",
    "The task is to evaluate passages of text after an edit was made to the knowledge of the natural language generation (edit made).\n",
    "The natural language generation system should generate text that is consistent with the edit made regardless of whether it is factual or not.\n",
    "\n",
    "Definitions:\n",
    "Edit Consistency: \n",
    "Is the passage consistent with the edit?\n",
    "Are all the statements in the passage consistent with itself?\n",
    "Are there any statements that agree with or disagree with the edit?\n",
    "Internal Consistency: \n",
    "Are the passages consistent with themselves?\n",
    "Regardless of consistency with edit, does the passage contradict itself?\n",
    "Cross Passage Consistency:\n",
    "Do the passages contradict each other?\n",
    "Cohesion/Topicality:\n",
    "Are the passages about the subject or related entity?\n",
    "Naturalness: \n",
    "Is the passage natural sounding text a native speaker would produce?\n",
    "Factuality: \n",
    "How close is the generated statement to the ground truth provided?\n",
    "\n",
    "Answer in the following format\n",
    "Edit Consistency: Number\n",
    "Internal Consistency: Number\n",
    "Cross Passage Consistency: Number\n",
    "Cohesion/Topicality: Number\n",
    "Naturalness: Number\n",
    "Factuality: Number\n",
    "\n",
    "Do not provide any commentary\n",
    "\"\"\"\n",
    "\n",
    "SURVEY_PROMPT = \"\"\"\n",
    "Survey:\n",
    "Edit Consistency: The passages are consistent with the edit made.\n",
    "Answer a number between 1 and 7\n",
    "\n",
    "Internal Consistency: The passages are consistent with themselves, regardless of the edit made.\n",
    "Answer a number between 1 and 7\n",
    "\n",
    "Cross Passage Consistency: The passages are consistent with each other, regardless of the edit made.\n",
    "Answer a number between 1 and 7\n",
    "\n",
    "Cohesion/Topicality: The passages are about the subject or related entity. They do not veer off topic\n",
    "Answer a number between 1 and 7\n",
    "\n",
    "Naturalness: The passage is natural sounding natural text a native speaker would produce.\n",
    "Answer a number between 1 and 7\n",
    "\n",
    "Factuality: The generated statement is completely consistent with the provided ground truth.\n",
    "Answer a number between 1 and 7\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greedy_no_edit\n",
      "Edit Consistency 1.6666666666666667\n",
      "Internal Consistency 6.555555555555555\n",
      "Cross Passage Consistency 4.444444444444445\n",
      "Cohesion/Topicality 6.888888888888889\n",
      "Fluency 7.0\n",
      "Factuality 1.8888888888888888\n",
      "\n",
      "greedy\n",
      "Edit Consistency 2.3333333333333335\n",
      "Internal Consistency 4.444444444444445\n",
      "Cross Passage Consistency 1.8888888888888888\n",
      "Cohesion/Topicality 4.555555555555555\n",
      "Fluency 7.0\n",
      "Factuality 1.2222222222222223\n",
      "\n",
      "sampled_no_edit\n",
      "Edit Consistency 1.7777777777777777\n",
      "Internal Consistency 6.666666666666667\n",
      "Cross Passage Consistency 5.111111111111111\n",
      "Cohesion/Topicality 7.0\n",
      "Fluency 7.0\n",
      "Factuality 1.8888888888888888\n",
      "\n",
      "sampled\n",
      "Edit Consistency 3.4444444444444446\n",
      "Internal Consistency 4.555555555555555\n",
      "Cross Passage Consistency 3.5555555555555554\n",
      "Cohesion/Topicality 6.555555555555555\n",
      "Fluency 6.888888888888889\n",
      "Factuality 1.4444444444444444\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "import time\n",
    "\n",
    "openai.api_key = \"\"\n",
    "\n",
    "EVAL_DIR = '../data/evaluation_samples/'\n",
    "EVAL_FILES = {\n",
    "    'greedy_no_edit': 'greedy_no_edit_samples.md',\n",
    "    'greedy': 'greedy_samples.md',\n",
    "    'sampled_no_edit': 'sampled_no_edit_samples.md',\n",
    "    'sampled': 'sampled_samples.md',\n",
    "}\n",
    "\n",
    "def get_survey_prompt(sample):\n",
    "    return sample + SURVEY_PROMPT\n",
    "    \n",
    "def _parse_scores(answer_text):\n",
    "    score_dict = {}\n",
    "    # answer text consists of\n",
    "    # label: value\n",
    "    # label: value\n",
    "    # ...\n",
    "\n",
    "    for line in answer_text.split('\\n'):\n",
    "        if line == '':\n",
    "            continue\n",
    "        label, value = line.split(':')\n",
    "        score_dict[label] = int(value)\n",
    "\n",
    "    return score_dict\n",
    "    \n",
    "scores_by_type = {}\n",
    "for eval_type, file_name in EVAL_FILES.items():\n",
    "    # open the file\n",
    "    with open(EVAL_DIR + file_name, 'r') as f:\n",
    "        samples = f.read().split('\\n## ')\n",
    "    overall_scores = {}\n",
    "    for sample in samples:\n",
    "        if sample.strip() == '':\n",
    "            continue\n",
    "        # get the prompt\n",
    "        prompt = get_survey_prompt(\"## \" + sample)\n",
    "        # get the answers\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": INSTRUCTION_PROMPT\n",
    "                },\n",
    "                {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "                },\n",
    "            ],\n",
    "            temperature=1,\n",
    "            max_tokens=256,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0\n",
    "        )\n",
    "        # parse the answers\n",
    "        answer_text = response['choices'][0]['message']['content']\n",
    "        scores = _parse_scores(answer_text)\n",
    "        # add to overall scores\n",
    "        for label, score in scores.items():\n",
    "            if label not in overall_scores:\n",
    "                overall_scores[label] = []\n",
    "            overall_scores[label].append(score)\n",
    "        # sleep for 1 second\n",
    "        time.sleep(10)\n",
    "    # print the overall scores\n",
    "    print(eval_type)\n",
    "    for label, scores in overall_scores.items():\n",
    "        print(label, sum(scores) / len(scores))\n",
    "    print()\n",
    "    scores_by_type[eval_type] = overall_scores\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
